{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "raw_data = open('breast-cancer-wisconsin.data','r')\n",
    "\n",
    "data=[]\n",
    "result=[]\n",
    "for i in raw_data:\n",
    "    tem_array = i.split(',')\n",
    "    tem_result = [0,0]\n",
    "    if len(tem_array) == 11:\n",
    "        if '?' in tem_array:\n",
    "            pass\n",
    "        else:\n",
    "            data.append(np.array(tem_array[1:-1]).astype(float))\n",
    "            tem_result[(int(tem_array[-1:][0].split('\\n')[0])-2)/2]=1\n",
    "            result.append( tem_result )\n",
    "\n",
    "data = map(lambda x: x/10.0, data)\n",
    "# print(len(data))\n",
    "# print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "479\n",
      "204\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "len_of_data = len(data)\n",
    "divide_train_test = int(0.7 * len_of_data)+1\n",
    "\n",
    "train_data = data[:divide_train_test]\n",
    "test_data = data[divide_train_test:]\n",
    "train_result = result[:divide_train_test]\n",
    "test_result = result[divide_train_test:]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(train_result))\n",
    "print(len(test_data))\n",
    "print(len(test_result))\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "train_data = list(chunks(train_data,50))\n",
    "train_result = list(chunks(train_result,50))\n",
    "len(train_result)\n",
    "\n",
    "combined = zip(train_data,train_result)\n",
    "random.shuffle(combined)\n",
    "\n",
    "train_data[:], train_result[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "dropout = 0.65 \n",
    " \n",
    "def weight_variable(shape):\n",
    "    initializer = tf.truncated_normal_initializer(dtype=tf.float32, stddev=1e-1)\n",
    "    return tf.get_variable(\"weights\", shape,initializer=initializer, dtype=tf.float32)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initializer = tf.constant_initializer(0.0)\n",
    "    return tf.get_variable(\"biases\", shape, initializer=initializer, dtype=tf.float32)\n",
    "\n",
    "\n",
    "original_graph = tf.Graph()\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess_original = tf.Session(graph = original_graph)\n",
    "with original_graph.as_default():\n",
    "    \n",
    "    num_class = 2\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=(None,9),name='x_')\n",
    "    y = tf.placeholder(tf.float32, (None, num_class),name='y_')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='d_') #dropout (keep probability)\n",
    "     \n",
    "\n",
    "    # fc1\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        W_fc1 = weight_variable([9, 100])\n",
    "        b_fc1 = bias_variable([100])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)\n",
    "\n",
    "    # dropout\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # fc2\n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        W_fc2 = weight_variable([100, 2])\n",
    "        b_fc2 = bias_variable([2])\n",
    "        y_conv = tf.Variable(0, name=\"y_conv\")\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    ans = tf.Variable(0, name=\"ans\")\n",
    "    ans = tf.argmax(y_conv,1)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "     \n",
    "    # Initializing the variables\n",
    "    #init = tf.initialize_all_variables()\n",
    "    init =tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver_ = tf.train.Saver()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Launch the graph\n",
    "sess_original.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.48\n",
      "0.7\n",
      "0.8\n",
      "0.84\n",
      "0.88\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.92\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "last_acc = 0\n",
    "c = 0\n",
    "for _ in range(0,100):\n",
    "    for i in range(0,9):\n",
    "        sess_original.run(optimizer, feed_dict={x: train_data[i], y: train_result[i], keep_prob: dropout})\n",
    "        acc = sess_original.run(accuracy, feed_dict={x: train_data[i], y: train_result[i], keep_prob: 1.})\n",
    "        \n",
    "    print(acc)\n",
    "            \n",
    "    if (acc - last_acc) < 0.01:\n",
    "        c += 1\n",
    "    last_acc = acc\n",
    "    \n",
    "    if c > 10:\n",
    "        break\n",
    "\n",
    "save_path = saver.save(sess, \"./model/model.ckpt\")\n",
    "        \n",
    "# #After Training, save the training status in graph\n",
    "# for variable in tf.trainable_variables():\n",
    "#     tensor = tf.constant(variable.eval())\n",
    "#     tf.assign(variable, tensor, name=\"nWeights\")\n",
    "#     #Save graph\n",
    "# tf.train.write_graph(sess_original.graph_def, 'model/', 'graph_vsd.pb', as_text=False)\n",
    "# print (\"save graph over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model/graph_vsd.pb'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = sess_original.run(accuracy, feed_dict={x: test_data, y: test_result, keep_prob: 1.})\n",
    "y = sess_original.run(ans, feed_dict={ x: test_data, keep_prob: 1. })\n",
    "print(acc)\n",
    "saver.save(sess_original, 'model/model',global_step=0)\n",
    "tf.train.write_graph(sess_original.graph_def, 'model/', 'graph_vsd.pb', as_text=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn.naive_bayes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-12ebb9f3913f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn.naive_bayes"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])  \n",
    "Y = np.array([1, 2, 3, 4, 5, 6])  \n",
    "from sklearn.naive_bayes import GaussianNB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
